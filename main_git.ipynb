{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightai.train import *\n",
    "from salt.resnet import *\n",
    "from salt.unet import *\n",
    "from salt.transform import *\n",
    "from salt.dataset import *\n",
    "from salt.metric import *\n",
    "from salt.file_op import *\n",
    "from salt.crit import *\n",
    "from salt.visualize import *\n",
    "from salt.predict import *\n",
    "from salt.callback import *\n",
    "from salt.evaluate import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distort = Distort(5,5,5)\n",
    "intensity = MyColorJitter(brightness=0.1)\n",
    "crop = CropRandom(0.7)\n",
    "param = {'degrees':0,'resample':Image.BICUBIC}\n",
    "zoom = MyRandomAffine(scale=[0.5,2],**param)\n",
    "zoom_in = MyRandomAffine(scale=[1,2],**param)\n",
    "zoom_out = MyRandomAffine(scale=[0.5,1],**param)\n",
    "shift = MyRandomAffine(translate=[0.5,0.5],**param)\n",
    "shear = MyRandomAffine(shear=45,**param)\n",
    "trn_tsfm = MyRandomApply([sample_hflip,shift,intensity,MyRandomChoice(\n",
    "        [distort,zoom,crop,shear],ps=[0.45,0.225,0.225,0.1])],ps=[0.5,0.5,0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "lr = 0.01\n",
    "wd = 5e-6\n",
    "drop = 0\n",
    "linear_drop = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = 'sample'\n",
    "k = 0\n",
    "print(f'fold {k}')\n",
    "trn_ds = CsvDataset(f'inputs/{file_loc}/{k}/trn.csv',tsfm=MyCompose(trn_tsfm,to_np))\n",
    "val_ds = CsvDataset(f'inputs/{file_loc}/{k}/val.csv',tsfm=MyCompose(to_np), tta_tsfms=[None, hflip])\n",
    "trn_sampler = BatchSampler(RandomSampler(trn_ds), bs, drop_last=True)\n",
    "val_sampler = BatchSampler(SequentialSampler(val_ds), bs, drop_last=False)\n",
    "trn_dl = DataLoader(trn_ds, trn_sampler)\n",
    "val_dl = DataLoader(val_ds, val_sampler)\n",
    "# log_dir = 'runs/step_lr'\n",
    "#     writer = SummaryWriter(f'{log_dir}')\n",
    "writer = None\n",
    "model = Dynamic(resnet34, trn_ds, drop, linear_drop, writer=writer).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "loss_fn = Crit([1,0.5,0.05])\n",
    "reverse_ttas = [None,hflip]\n",
    "metrics = [NoSaltScore(reverse_ttas), HasSaltScore(reverse_ttas), EmptyTP(reverse_ttas), EmptyFP(reverse_ttas),\n",
    "           Score(reverse_ttas)]\n",
    "evaluator = Evaluator(val_dl=val_dl,metrics=metrics,model=model,loss_fn=loss_fn)\n",
    "sv_best = SaveBestModel(model=model, optimizer=optimizer, small_better=False, name='best')\n",
    "# sv_period = SavePeriodically(period=5)\n",
    "learner = Learner(model=model, trn_dl=trn_dl, optimizer=optimizer, evaluator=evaluator, loss_fn=loss_fn, \n",
    "                  callbacks=[sv_best], metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "# sched = ReduceOnPlateau(optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5, verbose=True))\n",
    "# sched = LRSchedWrapper(optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.5))\n",
    "sched = LRSchedWrapper(optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5))\n",
    "# phase1 = np.linspace(1/20, 1, num=10, endpoint=False)\n",
    "# phase2 = np.linspace(1, 1/20, num=40)\n",
    "# phase = np.concatenate([phase1, phase2])\n",
    "# sched = LRSchedWrapper(optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: phase[epoch]))\n",
    "learner.fit(epochs, sched=sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if writer:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(1):\n",
    "    model = Dynamic(resnet34, trn_ds, drop, linear_drop, writer=writer).cuda()\n",
    "    model.load_state_dict(torch.load(f'model/best')['model'])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = []\n",
    "val_ds = CsvDataset(f'inputs/sample/2/val.csv',tsfm=MyCompose(to_np), tta_tsfms=[None,hflip])\n",
    "val_sampler = BatchSampler(val_ds, bs)\n",
    "val_dl = DataLoader(val_sampler, n_worker=4)\n",
    "for model in models:\n",
    "    s = val_score([model], val_dl, [None,hflip])\n",
    "    ss.append(s)\n",
    "print(np.array(ss).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_dls = []\n",
    "for i in range(5):\n",
    "    test_val_ds = CsvDataset(f'inputs/all/{i}/val.csv',tsfm=to_np, tta_tsfms=[None, hflip])\n",
    "    test_val_sampler = BatchSampler(test_val_ds, 128)\n",
    "    test_val_dl = DataLoader(test_val_sampler, n_worker=4)\n",
    "    test_val_dls.append(test_val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(5):\n",
    "    resnet = resnet18(drop=0)\n",
    "    model = Dynamic(resnet, trn_ds, 0, 0).cuda()\n",
    "    model.load_state_dict(torch.load(f'model/256ep lovasz loss/fold{i}')['model'])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ss = []\n",
    "for model, val_dl in zip(models, test_val_dls):\n",
    "    s = val_score(model, val_dl, [None, hflip])\n",
    "    ss.append(s)\n",
    "print(np.array(ss).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predict_test(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
